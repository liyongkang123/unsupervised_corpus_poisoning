## This code is generated by imitating from beir.retrieval.models import DPR, and to some extent it replaces DenseEncoderModel
from utils.beir_utils import DenseEncoderModel
from tevatron.retriever.modeling import EncoderOutput, DenseModel
from beir.retrieval.models import DPR
from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast
from typing import Union, List, Dict, Tuple
from tqdm.autonotebook import trange
import torch
from utils.utils import move_to_cuda

class RetrievalModel:
    def __init__(self, query_encoder, ctx_model, q_tokenizer,ctx_tokenizer, max_seq_len, max_query_len, pooling='cls', normalize=False):
        '''
        tokenizer: tokenizer for query and context, It must be the prefix_tokenizer I set
        '''
        # Query tokenizer and model
        self.q_tokenizer = q_tokenizer
        self.q_model = query_encoder
        self.q_model.cuda()
        self.q_model.eval()

        # Context tokenizer and model
        self.ctx_tokenizer = ctx_tokenizer
        self.ctx_model = ctx_model
        self.ctx_model.cuda()
        self.ctx_model.eval()

        self.pooling = pooling
        self.normalize = normalize

        if 'dpr' in  self.q_model.config.architectures[0].lower():
            self.dpr = True
        else:
            self.dpr = False

        self.max_seq_len = max_seq_len
        self.max_query_len= max_query_len

    def encode_queries(self, queries: List[str], batch_size: int = 16,show_progress: bool = True, **kwargs) -> torch.Tensor:
        query_embeddings = []
        with torch.no_grad():
            for start_idx in trange(0, len(queries), batch_size,disable=not show_progress): # trange is a wrapper of tqdm that can display a progress bar, but sometimes we don't want to display a progress bar.
                encoded = self.q_tokenizer(queries[start_idx:start_idx + batch_size], truncation=True, padding=True, max_length=self.max_query_len,
                                           return_tensors='pt',encode_query=True) #
                encoded = move_to_cuda(encoded)
                if self.dpr:
                    model_out = self.q_model(**encoded, return_dict=True)
                    emb = model_out.pooler_output #
                else:
                    model_out = self.q_model(**encoded, return_dict=True)
                    query_hidden_states = model_out.last_hidden_state
                    emb = self._pooling(query_hidden_states, encoded['attention_mask'])
                query_embeddings.append(emb.cpu())
            allemb = torch.cat(query_embeddings, dim=0).cpu().numpy() # Beir's DenseEncoderModel uses numpy, so it returns numpy uniformly.
        return allemb

    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size: int = 8, show_progress: bool = True, **kwargs) -> torch.Tensor:
        corpus_embeddings = []
        with torch.no_grad():
            for start_idx in trange(0, len(corpus), batch_size,disable=not show_progress): #
                titles = [row['title'] for row in corpus[start_idx:start_idx + batch_size]]
                texts = [row['text'] for row in corpus[start_idx:start_idx + batch_size]]
                combined_inputs = [f'{title.strip()} {text.strip()}'.strip() for title, text in zip(titles, texts)]
                encoded = self.ctx_tokenizer(combined_inputs, truncation=True, max_length=self.max_seq_len, padding=True,return_tensors='pt', encode_passage=True) # encode_passage=True means that the input is a passage

                encoded = move_to_cuda(encoded)
                if self.dpr:
                    model_out = self.ctx_model(**encoded, return_dict=True)
                    emb = model_out.pooler_output # batch_size * 768
                else:
                    model_out = self.ctx_model(**encoded, return_dict=True)
                    query_hidden_states = model_out.last_hidden_state
                    emb = self._pooling(query_hidden_states, encoded['attention_mask'])
                corpus_embeddings.append(emb.cpu())
            allemb = torch.cat(corpus_embeddings, dim=0).cpu().numpy()
        return allemb

    def _pooling(self, last_hidden_state, attention_mask):

        if self.pooling in ['cls', 'first']:
            reps = last_hidden_state[:, 0]
        elif self.pooling in ['mean', 'avg', 'average']:
            masked_hiddens = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)
            reps = masked_hiddens.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
        elif self.pooling in ['last', 'eos']:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden_state.shape[0]
            reps = last_hidden_state[torch.arange(batch_size, device=last_hidden_state.device), sequence_lengths]
        elif self.pooling in ['all']:
            reps = last_hidden_state
        else:
            raise ValueError(f'unknown pooling method: {self.pooling}')

        if self.normalize: # If set to True, the cosine similarity is calculated.
            reps = torch.nn.functional.normalize(reps, p=2, dim=-1)
        return reps